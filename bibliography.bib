--------- Misc. ---------
// Action Research
@article{baskerville1999investigating,
  title={Investigating information systems with action research},
  author={Baskerville, Richard L},
  journal={Communications of the association for information systems},
  volume={2},
  number={1},
  pages={19},
  year={1999}
}

@article{cornish2023participatory,
  title={Participatory action research},
  author={Cornish, Flora and Breton, Nancy and Moreno-Tabarez, Ulises and Delgado, Jenna and Rua, Mohi and de-Graft Aikins, Ama and Hodgetts, Darrin},
  journal={Nature Reviews Methods Primers},
  volume={3},
  number={1},
  pages={34},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{bierman2014understanding,
  title={Understanding typescript},
  author={Bierman, Gavin and Abadi, Mart{\'\i}n and Torgersen, Mads},
  booktitle={European Conference on Object-Oriented Programming},
  pages={257--281},
  year={2014},
  organization={Springer}
}
--------- EAM ---------

@book{jung2021masterclass,
  title={Masterclass Enterprise Architecture Management},
  author={Jung, J{\"u}rgen and Fraunholz, Bardo},
  year={2021},
  publisher={Springer}
}

// Paper von Jürgen aus 2019, weshalb EAM praktisch ist
@inproceedings{jung2019purpose,
  title={Purpose of enterprise architecture management: investigating tangible benefits in the german logistics industry},
  author={Jung, J{\"u}rgen},
  booktitle={2019 IEEE 23rd International Enterprise Distributed Object Computing Workshop (EDOCW)},
  pages={25--31},
  year={2019},
  organization={IEEE}
}

// Diese Quelle baut auf die oben drüber auf - geht ins Detail was weitere Herausforderungen im Alltag eines EAs sind
@article{castro2021towards,
  title={Towards Measuring Success of Enterprise Architecture Decisions: Survey among Practitioners and Outline of a Framework},
  author={Castro, Sandra and Jung, J{\"u}rgen},
  journal={Enterprise Architecture Professional Journal},
  pages={1--22},
  year={2021}
}

// Quelle als kleiner Joke, dass ITler gerne Technologie implementieren weil sie die Technologie einfach geil finden.
@article{ahmed2017motivating,
  title={Motivating information technology professionals: The case of New Zealand},
  author={Ahmed, Shoaib and Taskin, Nazim and Pauleen, David J and Parker, Jane and others},
  journal={Australasian Journal of Information Systems},
  volume={21},
  year={2017},
  publisher={Australasian Association for Information Systems}
}


--------- LLMs and Conversational Agents ---------

@software{openai_chatgpt_2023,
  author       = {{OpenAI}},
  year         = {2023},
  title        = {ChatGPT (January 06 version)},
  howpublished = {\url{https://chat.openai.com/chat}},
  note         = {[Large language model]}
}

@article{gu2024survey,
  title={A survey on llm-as-a-judge},
  author={Gu, Jiawei and Jiang, Xuhui and Shi, Zhichao and Tan, Hexiang and Zhai, Xuehao and Xu, Chengjin and Li, Wei and Shen, Yinghan and Ma, Shengjie and Liu, Honghao and others},
  journal={arXiv preprint arXiv:2411.15594},
  year={2024}
}

@misc{li2025chatsopsopguidedmctsplanning,
      title={ChatSOP: An SOP-Guided MCTS Planning Framework for Controllable LLM Dialogue Agents}, 
      author={Zhigen Li and Jianxiang Peng and Yanmeng Wang and Yong Cao and Tianhao Shen and Minghui Zhang and Linxi Su and Shang Wu and Yihang Wu and Yuqian Wang and Ye Wang and Wei Hu and Jianfeng Li and Shaojun Wang and Jing Xiao and Deyi Xiong},
      year={2025},
      eprint={2407.03884},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.03884}, 
}

@ARTICLE{Zhai_2025,
	title={A Survey of Task Planning with Large Language Models},
	year={2025},
	author={Wenshuo Zhai and Jinzhi Liao and Ziyang Chen and Bolun Su and Xiang Zhao},
	doi={10.34133/icomputing.0124},
	pmid={null},
	pmcid={null},
	mag_id={null},
	journal={Intelligent Computing},
	abstract={null}
}

@misc{deng2023promptingevaluatinglargelanguage,
      title={Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration}, 
      author={Yang Deng and Lizi Liao and Liang Chen and Hongru Wang and Wenqiang Lei and Tat-Seng Chua},
      year={2023},
      eprint={2305.13626},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13626}, 
}

@article{hadi2023large,
  title={Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects},
  author={Hadi, Muhammad Usman and Qureshi, Rizwan and Shah, Abbas and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali and others},
  journal={Authorea preprints},
  volume={1},
  number={3},
  pages={1--26},
  year={2023},
  publisher={Authorea}
}

@article{singhal2023large,
  title={Large language models encode clinical knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={172--180},
  year={2023},
  publisher={Nature Publishing Group}
}

ephemeral conversation memory
@article{zeppieri2025mmag,
  title={MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications},
  author={Zeppieri, Stefano},
  journal={arXiv preprint arXiv:2512.01710},
  year={2025}
}

@article{buchmann2024large,
  title={Large language models: Expectations for semantics-driven systems engineering},
  author={Buchmann, Robert and Eder, Johann and Fill, Hans-Georg and Frank, Ulrich and Karagiannis, Dimitris and Laurenzi, Emanuele and Mylopoulos, John and Plexousakis, Dimitris and Santos, Maribel Yasmina},
  journal={Data \& Knowledge Engineering},
  volume={152},
  pages={102324},
  year={2024},
  publisher={Elsevier}
}


---------Neo4j, RAG and everything database ---------

@book{robinson2015graph,
  title={Graph databases: new opportunities for connected data},
  author={Robinson, Ian and Webber, Jim and Eifrem, Emil},
  year={2015},
  publisher={"O'Reilly Media, Inc."}
}

@book{van2014learning,
  title={Learning Neo4j},
  author={Van Bruggen, Rik},
  year={2014},
  publisher={Packt Publishing Ltd}
}

@article{wang2017knowledge,
  title={Knowledge graph embedding: A survey of approaches and applications},
  author={Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li},
  journal={IEEE transactions on knowledge and data engineering},
  volume={29},
  number={12},
  pages={2724--2743},
  year={2017},
  publisher={IEEE}
}

@article{zhao2026retrieval,
  title={Retrieval-augmented generation for ai-generated content: A survey},
  author={Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Jiang, Jie and Cui, Bin},
  journal={Data Science and Engineering},
  pages={1--29},
  year={2026},
  publisher={Springer}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@misc{temporalGraphRag,
      title={T-GRAG: A Dynamic GraphRAG Framework for Resolving Temporal Conflicts and Redundancy in Knowledge Retrieval}, 
      author={Dong Li and Yichen Niu and Ying Ai and Xiang Zou and Biqing Qi and Jianxing Liu},
      year={2025},
      eprint={2508.01680},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2508.01680}, 
}

@article{wan2025prompting,
  title={Prompting large language models based on semantic schema for text-to-Cypher transformation towards domain Q\&A},
  author={Wan, Yuwei and Chen, Zheyuan and Liu, Ying and Chen, Chong and Packianather, Michael},
  journal={Decision Support Systems},
  pages={114553},
  year={2025},
  publisher={Elsevier}
}

@inproceedings{mihindukulasooriya2023text2kgbench,
  title={Text2kgbench: A benchmark for ontology-driven knowledge graph generation from text},
  author={Mihindukulasooriya, Nandana and Tiwari, Sanju and Enguix, Carlos F and Lata, Kusum},
  booktitle={International semantic web conference},
  pages={247--265},
  year={2023},
  organization={Springer}
}

@article{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2111.02080},
  year={2021}
}


--------- Prototypes ---------


@unpublished{MsGraphRAGPrototype,
author = {Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Metropolitansky, Dasha and Ness, Robert Osazuwa and Larson, Jonathan},
title = {From Local to Global: A Graph RAG Approach to Query-Focused Summarization},
year = {2024},
month = {April},
abstract = {The use of retrieval-augmented generation (RAG) to retrieve relevant information from an external knowledge source enables large language models (LLMs) to answer questions over private and/or previously unseen document collections. However, RAG fails on global questions directed at an entire text corpus, such as "What are the main themes in the dataset?", since this is inherently a query-focused summarization (QFS) task, rather than an explicit retrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of text indexed by typical RAG systems. To combine the strengths of these contrasting methods, we propose GraphRAG, a graph-based approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text. Our approach uses an LLM to build a graph index in two stages: first, to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that GraphRAG leads to substantial improvements over a conventional RAG baseline for both the comprehensiveness and diversity of generated answers.},
url = {https://www.microsoft.com/en-us/research/publication/from-local-to-global-a-graph-rag-approach-to-query-focused-summarization/},
}

@misc{MsGraphRAGPaper,
      title={From Local to Global: A Graph RAG Approach to Query-Focused Summarization}, 
      author={Darren Edge and Ha Trinh and Newman Cheng and Joshua Bradley and Alex Chao and Apurva Mody and Steven Truitt and Dasha Metropolitansky and Robert Osazuwa Ness and Jonathan Larson},
      year={2025},
      eprint={2404.16130},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.16130}, 
}



--------- Evaluation ---------


@inproceedings{evaluationFrameworkLLMs,
author = {Wolters, Anna and Arz von Straussenburg, Arnold and Riehle, Dennis M.},
year = {2024},
month = {07},
pages = {},
title = {Evaluation Framework for Large Language Model-based Conversational Agents}
}

@article{liu2023agentbench,
  title={Agentbench: Evaluating llms as agents},
  author={Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and others},
  journal={arXiv preprint arXiv:2308.03688},
  year={2023}
}


--------- Literature Review / Current state of the art ---------

@article{brocke2009reconstructing,
  title={Reconstructing the giant: On the importance of rigour in documenting the literature search process},
  author={Brocke, Jan vom and Simons, Alexander and Niehaves, Bjoern and Niehaves, Bj{\"o}rn and Riemer, Kai and Plattfaut, Ralf and Cleven, Anne},
  year={2009}
}

@article{webster2002analyzing,
  title={Analyzing the past to prepare for the future: Writing a literature review},
  author={Webster, Jane and Watson, Richard T},
  journal={MIS quarterly},
  pages={xiii--xxiii},
  year={2002},
  publisher={JSTOR}
}

@article{page2021prisma,
  title={The PRISMA 2020 statement: an updated guideline for reporting systematic reviews},
  author={Page, Matthew J and McKenzie, Joanne E and Bossuyt, Patrick M and Boutron, Isabelle and Hoffmann, Tammy C and Mulrow, Cynthia D and Shamseer, Larissa and Tetzlaff, Jennifer M and Akl, Elie A and Brennan, Sue E and others},
  journal={bmj},
  volume={372},
  year={2021},
  publisher={British Medical Journal Publishing Group}
}

@article{mishra2023prisma,
  title={PRISMA for review of management literature--method, merits, and limitations--an academic review},
  author={Mishra, Vinaytosh and Mishra, Monu Pandey},
  journal={Advancing methodologies of conducting literature review in management domain},
  pages={125--136},
  year={2023},
  publisher={Emerald Publishing Limited}
}


--------- Compilers / Transpilers / Parsers ---------


@article{ruhlemann2017conversation,
  title={Conversation Analysis and the XML method},
  author={R{\"u}hlemann, Christoph and Gee, Matt},
  journal={Gespr{\"a}chsforschung},
  year={2017},
  publisher={Arnulf Deppermann and Martin Hartung}
}

@misc{clark1999xml,
  title={XML path language (XPath)},
  author={Clark, James and DeRose, Steve and others},
  year={1999}
}


--------- MCP ---------


@article{hou2025model,
  title={Model context protocol (mcp): Landscape, security threats, and future research directions},
  author={Hou, Xinyi and Zhao, Yanjie and Wang, Shenao and Wang, Haoyu},
  journal={arXiv preprint arXiv:2503.23278},
  year={2025}
}


--------- Internet Sources ---------

@online{neo4j_apoc_meta_schema,
  author       = {{Neo4j, Inc.}},
  title        = {apoc.meta.schema},
  year         = {2025},
  url          = {https://neo4j.com/labs/apoc/4.1/overview/apoc.meta/apoc.meta.schema/},
  urldate      = {2026-01-01},
  note         = {Technical documentation for the APOC library in Neo4j}
}

@online{neo4j_apoc_load_xml,
  author       = {{Neo4j, Inc.}},
  title        = {Load XML},
  year         = {2025},
  url          = {https://neo4j.com/labs/apoc/4.2/import/xml/},
  urldate      = {2026-01-01},
  note         = {Technical documentation for the apoc.load.xml procedure in Neo4j}
}

@online{neo4j_cypher_indexes,
  author       = {{Neo4j, Inc.}},
  title        = {Indexes},
  year         = {2025},
  url          = {https://neo4j.com/docs/cypher-manual/current/indexes/},
  urldate      = {2026-01-01},
  note         = {Cypher manual documentation on index definition and usage in Neo4j}
}

@online{neo4j_operations_procedures,
  author       = {{Neo4j, Inc.}},
  title        = {Procedures},
  year         = {2025},
  url          = {https://neo4j.com/docs/operations-manual/current/procedures/#procedure_db_schema_visualization},
  urldate      = {2026-01-01},
  note         = {Neo4j Operations Manual documentation including the \texttt{db.schema.visualization} procedure}
}

@online{openai_api,
  author       = {{OpenAI}},
  title        = {OpenAI API Documentation},
  year         = {2025},
  url          = {https://platform.openai.com/docs/overview},
  urldate      = {2026-01-01},
  note         = {Technical documentation for accessing large language models via the OpenAI API}
}

MCP Anthropic
@online{anthropic_mcp_2025,
  author  = {{Anthropic PBC}},
  title   = {Introducing the Model Context Protocol},
  year    = {2025},
  url     = {https://www.anthropic.com/news/model-context-protocol},
  urldate = {2026-01-02}
}

@software{neo4j_mcp_2025,
  author  = {{Neo4j, Inc.}},
  title   = {Neo4j MCP Server},
  year    = {2025},
  url     = {https://github.com/neo4j/mcp},
  note    = {GitHub repository},
  urldate = {2026-01-02}
}


@misc{qwen2.5,
    title = {Qwen2.5: A Party of Foundation Models},
    url = {https://qwenlm.github.io/blog/qwen2.5/},
    author = {Qwen Team},
    month = {September},
    year = {2024}
}

@online{llamaindex_llamaparse_2025,
  author  = {{LlamaIndex}},
  title   = {Parsing PDFs with LlamaParse: a how-to guide},
  year    = {2025},
  url     = {https://www.llamaindex.ai/blog/pdf-parsing-llamaparse},
  urldate = {2026-01-02}
}

@software{yu_graph_rag_2025,
  author  = {Yu, Joshua},
  title   = {graph-rag},
  year    = {2025},
  url     = {https://github.com/Joshua-Yu/graph-rag},
  note    = {GitHub repository},
  urldate = {2026-01-02}
}

@misc{chakra_ui,
  title        = {{Chakra UI} is a component system for building products},
  author       = {{Chakra Systems}},
  year         = {2026},
  howpublished = {\url{https://chakra-ui.com/}},
  note         = {Online; accessed 2026}
}

@software{neo4j_2026,
  author       = {{Neo4j, Inc.}},
  year         = {2026},
  title        = {Neo4j Graph Database},
  howpublished = {\url{https://neo4j.com/}},
  note         = {Accessed 2026}
}

@misc{atlassian2026rovo,
  title        = {Meet Rovo, {AI} that knows your business},
  author       = {{Atlassian}},
  year         = {2026},
  howpublished = {\url{https://www.atlassian.com/software/rovo}},
  note         = {Accessed: 2026-01-27}
}

