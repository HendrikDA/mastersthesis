\chapter{Review Workshop With Experts}
\label{ch:case_study}
With an understanding of the prototype's architecture from chapter \ref{ch:implementation}, this chapter dives into the test cases that \prototype{} was put through during the final review from section \ref{sec:action_research_final_meeting}. This chapter describes the context of the case study, how the system was prepared, the tests executed, and how everything was evaluated.

% A structured, factual description of how Masuta was applied in a concrete setting and what was observed during its use.

% Case Context
\section{Case Context}
\prototype{} was put to the test by two domain experts within the realm of enterprise architecture management. This ensured that the questions being asked of the system mirrored a real world setting. The goal of the conducted case study was to evaluate the final system end-to-end and compare its results to a similar, mainstream system. The conducted tests were done in a qualitative and explorative manner, meaning that the focus of the case study was placed on the system's usefulness and perceived value from the experts' point of view.

This qualitative approach was deemed appropriate, as the system was developed as a proof of concept to support enterprise architects in their daily doing. A quantitative test scenario was not considered because the prototype was not developed with explicit quantitative requirements, for example response times within \textit{x} seconds. The practitioners interacted with both \prototype{} and Rovo in parallel while actively comparing results for similar prompts. The comparison was meant to reveal strengths and weaknesses within \prototype{}.

Questions asked towards both systems were constructed to cover all three categories of questions as well attempt to expand the existing architecture with further information.

% System Setup and Data
\section{System Setup and Data}
The finished prototype system as described in chapter \ref{ch:implementation} was used throughout. \prototype{} was installed on both practitioners computers and came out of the box with the SpeedParcel example dataset and the empty playground database which only contained the textbook knowledge. All components of \prototype{}, with the exception of the LLM, ran on the local environments of the practitioners' computers.

In order to simulate a real world setting, the enterprise architecture for the examination office of Frankfurt University of Applied Science was used. This architecture included several diagrams within the Archi modeling tool, ranging from high-level process landscapes to detailed views of examination preparation, thesis handling, certification assurance, and IT systems used by the examination office. Models were exported into XML format and read into \prototype{}'s playground database as described in section \ref{sub:xml_parser} and appendix section \ref{appendix:instructions:import}. Frankfurt University of Applied Science's architecture is described in German. The imported data from the examination office's architecture diagrams contained a total of 92 nodes and 152 edges.

Parallel to \prototype{}, Rovo was being used with the same ingested architecture data loaded into Confluence. This allowed the practitioners to test the same dataset with a second system. It included the same documentation and architecture data as found in Archi. 

% Case Execution
\section{Case Execution}
What did users actually do?
How questions were asked
Categories of questions (your 3 levels)
Who asked which type of question
Sequence:
- User prompt
- Schema call
- Retrieval
- Cypher generation
- Answer
- Rovo comparisson

You can include:
- Representative example questions
- Short transcripts (possibly anonymized)
- No judgment


% Observed System Behavior
\section{Observed System Behavior}
What did the system output or do?
Put here:
- Correct answers
- Empty result sets
- Hallucinated outputs
- Cypher generation attempts
- Hardcoded Cypher cases
 Read-only write attempts
- Latency observations
- Differences between expert vs. novice prompts
Example: “In several cases, the system generated Cypher queries that did not reference the retrieved graph elements but instead returned static text.”

% Summary of Case Observations
\section{Summary of Case Observations}
Bullet-point summary of:
- What was observed
- What patterns appeared
No evaluation language
Examples:
“Successful retrieval occurred primarily when node types were explicitly mentioned.”
“Schema ambiguity frequently led to empty result sets.”


----------------------------------------------


What we did with the finished prototype and how we tested it. Prompts we used, cases we built, edge cases, etc.

show some test cases here. break down an answer like "i want to remove the application StatManPlus. What do i have to look out for as an enterprise architect?". the returned answer goes into a lot of depth as this is a level 3 question. break down the result of the query and how the result pulls information about the application but also pulls information from the Lehrbuch database.


Idea: create a list of questions over each category with expected answers and see how it performs. also run x amount of experiments and see how many errors came up (e.g. cypher errors) and count the percent of answers that were perfect, good but could be improved, and wrong / faulty.

