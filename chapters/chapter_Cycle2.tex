\chapter{Cycle 2}
\label{ch:cycle2}
The second cycle builds upon the foundation laid during the first and marked the transition from a purely technical proof of concept toward a more structurally grounded and application-oriented prototype, thereby clarifying what the path toward a finished prototype might look like.

% Diagnosis
\section{Diagnosis}
\label{sec:cycle2_diagnosis}
Following the initial feasibility assessment, the next identified challenge concerned extending the knowledge graph with additional information and positioning the prototype as a practically useful tool within the domain of enterprise architecture management. This required integrating the first elements of architectural data into the knowledge graph, as well as addressing the shortcomings of the existing querying method and LLM runtime performance.

One design consideration involved separating textbook-based domain knowledge from enterprise architecture data into two distinct databases. After discussion, a single integrated database was chosen in order to enable direct relationships between conceptual textbook knowledge and architectural artifacts. This decision reflects the assumption that conceptual and structural knowledge should not remain isolated, but instead mutually reinforce contextual reasoning within the system when answering a user's prompt.

The challenge imposed by lack of real-world data to test the system was also identified. Potential test datasets were discussed in order to bridge the gap before proprietary enterprise data could be incorporated. To support development, it was agreed that the data from a completed university assignment from the Frankfurt University of Applied Sciences be used, consisting of an application landscape, business capability map, business capability support matrix, business object model, and cross-application data-flow diagram. The dataset stems from a fictitious logistics company named \textit{SpeedParcel}. Although fictitious, it provides a sufficiently complex architectural scenario to test structural integration and querying logic under realistic constraints This SpeedParcel dataset will be used throughout the next cycles.

% Action Planning
\section{Action Planning}
\label{sec:cycle2_planning}
The objective of the second cycle was to extend the knowledge graph with additional data from the SpeedParcel dataset. This includes integrating SpeedParcel's business capability support matrix into the knowledge graph, which is in the format of an Excel file. To prepare this data for the knowledge graph, a parser will be written that is able to transform the architecture diagrams into nodes and relationships.

A further objective is to improve the querying method. Alternative approaches were to be explored during this phase in order to evaluate whether the embedding-based retrieval method was appropriate, or whether alternative retrieval strategies would prove more suitable. This exploration was guided by the shortcomings identified during Cycle 1, particularly regarding the graph querying method. In addition, alternative LLM models were to be implemented with respect to response quality, latency, and maintainability.

% Action Taken
\section{Action Taken}
\label{sec:cycle2_actionTaken}
The transformation of SpeedParcel's capability support matrix was approached by developing a customized parser, specifically written for the Excel file containing the capability support matrix. The parser was developed by prompting an LLM with a description of the data structure and iteratively refining a Python script capable of reading the Excel file, decomposing it according to the predefined schema, and inserting the resulting nodes and relationships into Neo4j. This approach required iterative validation of the inserted graph structures to ensure consistency with the original architectural model. After several rounds of fine tuning, the capability support matrix could be read into the knowledge graph.

In order to solve the previous challenge of overreliance on an LLM when developing, it was decided by the researcher that the backend will be rewritten in Node.js\footnote{Node.js: \url{https://nodejs.org/en}}, as the researcher is more versed in JavaScript backends than in Python. This rewrite did not change the core logic of the application and still allowed for typical client-server HTTP communication between the frontend and backend. However, this refactoring enabled a more maintainable codebase and improved transparency at a code level, thereby reducing development dependency on AI-assisted code generation. This decision also reflects a shift toward long-term architectural sustainability and scalability rather than short-term development speed.

During the backend refactoring, the idea of a Model Context Protocol (MCP) server came up during development. Because Neo4j offers MCP support\footnote{Neo4j MCP: \url{https://neo4j.com/docs/mcp/current/}} via an APOC plugin, creating the MCP server and connecting it to the Node.js backend simply required executing predefined functions. This brought the advantage of a standardized interface between the application and the knowledge graph, abstracting direct database interaction.

However, the inappropriate querying method remained a challenge that needed to be overcome. An approach was tried out in which the user's input was used as context in a prompt towards an LLM which is tasked with generating Cyphers to retrieve the information that the user is requesting. Rather than generating embeddings and performing similarity-based retrieval, the user's input was now used as contextual input for an LLM tasked with generating explicit Cypher queries. This text-to-Cypher approach requires that a system prompt be passed as context to the LLM, supplementing the user's input with additional information and instructions of what the LLM is being tasked with. For example, the first version of the system prompt gives the LLM information about the knowledge graph's structure, including the structure of the textbook and the capability support matrix. It is also given basic instructions for the task of creating Cyphers based off of the user input for the specified knowledge graph structure. The LLM uses the system prompt alongside the user's input in order to generate Cyphers queries tailored to the underlying graph schema. This approach shifts away from a similarity search toward graph traversal.

Implementing the MCP server along with the text-to-Cypher approach helps achieve a higher quality in the retrieved answers, as the user's prompt is now being transformed into custom Cyphers with the help of the system prompt. This allows the Cyphers being used to query the database instead of filling out the parameters of a hard-coded Cypher as before. The MCP server brings the advantage of executing these Cyphers along a standardized interface, rather than by connecting the backend directly to the database and executing the Cyphers directly on the database.

Lastly, the Qwen model was replaced with the latest GPT-5.2\footnote{Open AI GPT-5.2: \url{https://developers.openai.com/api/docs/models/gpt-5.2}} model from Open AI. This change required replacing all calls to the previous model with calls to Open AI's API. The advantage of using such a mainstream model is the increased speed and quality in generated answers as well as the improved development support due to more documentation and a higher expected lifetime of the model. However, this shift introduces per-request operational costs and reduces full local deployability. This trade-off reflects a prioritization of response quality and development reliability, as this research is concerned with developing a mature prototype rather than an economically viable product.

% Evaluation
\section{Evaluation}
\label{sec:cycle2_evaluation}
The matured prototype was again evaluated qualitatively through a demonstration by the researcher and open discussions between all three stakeholders. Both advisors expressed strong approval of the direction taken and confirmed that the prototype was evolving toward a practically viable solution in order to achieve the end goal of allowing enterprise architects to interact with a proprietary knowledge graph via natural language. The prototype successfully demonstrated improvements in both conceptual and structural question answering. Conceptual questions within the domain of EAM were answered with improved consistency, while structural questions targeting the SpeedParcel dataset were handled more reliably. The results with the new text-to-Cypher approach along with the MCP server significantly improved retrieval precision compared to the embedding-based method from the previous cycle. The new LLM being used also generates answers within seconds, rather than minutes, and does so with a higher quality natural language output.

A prerequisite for the Cypher generation to work is prior knowledge of the knowledge graph schema. At this stage, the schema is hard-coded within the backend, which limits scalability as this will require adapting the system context to further datasets later added to the knowledge graph. This hard-coded mechanism is noted as a potential bottleneck as it introduces a dependency that may bottleneck the knowledge graph's scalability when dealing with more heterogeneous datasets further down the line.

It was during this evaluation that the practitioners suggested that Action Research be applied as the research method for the explorative development process. This was deemed fitting because the approach so far has by design strongly resembled that of Action Research. Adapting to formal Action Research simply meant introducing academic conditions into the approach, such as documenting discussions with the goal of reproducing the efforts within this thesis. Committing to Action Research also meant that the focus can continue to be laid on developing the prototype, as opposed to spending time and effort laying out how the system may be tested. This allowed for a longer development phase within the timeframe of the thesis, enabling a more well rounded prototype to be strived for as the finished product. Applying Action Research also implies that the practitioners are not merely responsibility for evaluating the current implementation, but are active co-creators responsible for generating ideas on how to further evolve the system.

During the discussions, it was regularly mentioned that enterprise architects work on architectural artifacts within the application Archi\footnote{Archi: \url{https://www.archimatetool.com/}}, which as a tool allows enterprise architects to graphically represent enterprise architectures as diagrams. This means that moving forward, a requirement of the system is to be able to import and interpret the enterprise architecture diagrams exported from Archi.

It was also during these discussions where a deadline for the prototype development was agreed upon. Alongside the deadline, a final review was discussed where all three stakeholders will be testing and evaluating the system together. This also means that the prototype must be prepared for such a final review. The use case of a final review requires that a local deployment be setup in order to allow the other practitioners to have the prototype running on their local computers. It was agreed up to use an approach with Docker\footnote{Docker: \url{https://www.docker.com/}}, as this allows each system component to be added to an encapsulated container, enabling an operating system agnostic execution method of the prototype.

Furthermore, the practitioners and researcher identified three key categories of user questions. The first category identified conceptual questions related to enterprise architecture principles, concepts, and best practices found in textbooks. The second category contains descriptive questions targeting concrete architectural elements and relationships. The third and final category contains integrative questions combining conceptual knowledge with specific architectural contexts. Examples of these categories can be found in the Appendix's chapter \ref{appendix:ch:example_questions}. This overlaps with the categories identified in section \ref{sec:intro:relatedWork} by Zhao et al. (2024) \cite{zhao2024retrieval}, where explicit facts map to the identified category 1, implicit facts map to category 2, and the interpretable rationales and hidden rationales map to category 3. This categorization provides a structured evaluation framework for the final review, as this allows the system's maturity to be assessed along progressively complex reasoning dimensions.

The practitioners also expressed and envisioned practical use cases for the finished prototype in which an EAM project plan is developed with the support of the developed chatbot. For example, the chatbot may support in identifying which applications are redundant in an enterprise's architecture. Subsequent analysis of the business capabilities influenced by this change can be supported by the chatbot as well. This confirms that the prototype has progressed beyond experimental validation toward plausible practical applicability. Although the development is still exploratory, the system demonstrates early indications of value creation within typical workflows of enterprise architects.

% Learning
\section{Learning}
\label{sec:cycle2_learning}
As the prototype matured, clearer insights emerged from the second cycle.

Firstly, the previously implemented embedding-based retrieval method proved insufficient for dynamic interaction with the evolving graph structure. While similarity search may be suitable when dealing with unstructured textual knowledge, it became evident that structured enterprise architecture data benefits more from a traversal-based querying approach. The text-to-Cypher implementation demonstrated that generating explicit graph queries based on the userâ€™s input allows the system to use the structural properties of the knowledge graph. This marks a conceptual shift toward a more controlled graph traversal, reinforcing the importance of schema awareness when combining LLMs with structured data.

The switch to a mainstream LLM further highlighted the dependency of the underlying model being used. While the use of GPT-5.2 reduced full local deployability and introduced operational costs, it significantly improved response latency and output quality. This shift illustrates that the development of a mature prototype may require prioritizing stability and performance over complete autonomy, especially when the objective is to evaluate conceptual viability rather than economic feasibility.

The introduction of the MCP server increased modularity and abstraction between system components, reducing coupling between backend logic and database interaction. This architectural decoupling represents an important step toward scalability. It became clear that as the knowledge graph grows and integrates additional datasets, clear interfaces between components are necessary to prevent the system from becoming rigid or overly dependent on hard-coded structures. The MCP server also allows further services than just Neo4j to be connected in a standardized way in future development.

Methodologically, the formal adoption of Action Research in this cycle clarified the collaborative nature of the development process. The practitioners did not merely evaluate the prototype but actively influenced its direction, for example by introducing the requirement to support Archi exports and by defining structured categories of user questions. The categorization of conceptual, descriptive, and integrative questions also provided a clearer lens through which system maturity can be assessed in subsequent cycles.

Finally, the broader discussions during this cycle revealed a gradual transformation in how the prototype is perceived. What began as an experimental knowledge retrieval system is increasingly viewed as a potential decision-support tool for enterprise architecture projects. This shift in perception indicates that the system is moving beyond isolated technical experimentation toward integration within realistic architectural workflows. Although the development remains exploratory, the second cycle demonstrated that structured graph integration combined with controlled LLM orchestration provides a viable foundation for further refinement in the subsequent cycles.
