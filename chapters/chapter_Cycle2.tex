\chapter{Cycle 2}
\label{ch:cycle2}
The second cycle builds upon the foundation laid during the first and is the shortest cycle of the four. However, the second cycle allowed extending the proof of concept from the first cycle into a more mature version and makes clear what the path toward a finished prototype version might look like.

% Diagnosis
\section{Diagnosis}
\label{sec:cycle2_diagnosis}
Following the initial feasibility assessment, the next identified challenge concerned extending the knowledge graph with more information and positioning the prototype as a useful tool within the realm of enterprise architecture management. This means adding the first bits of architectural data to the knowledge graph as well as improving the shortcomings of the querying method and LLM runtime.

One design consideration involved separating textbook-based domain\linebreak knowledge from enterprise architecture data into two distinct databases. After discussions, a single integrated database was agreed upon to enable direct relationships between conceptual textbook knowledge and architectural data, with the expectation of improved contextual reasoning.

The challenge imposed by lack of real-world data to test the system was also identified. Potential test datasets for further development and evaluation were discussed in order to bridge the gap before real-world data could be applied. To aid development, it was agreed upon that the data from a completed university assignment from the Frankfurt University of Applied Sciences be used, consisting of an application landscape, business capability map, business capability support matrix, business object model, and cross-application data-flow diagram for a fictitious logistics company named \textit{SpeedParcel}. This SpeedParcel dataset will be used throughout the next cycles.

% Action Planning
\section{Action Planning}
\label{sec:cycle2_planning}
The objective of the second cycle is to extend the knowledge graph with additional data from the SpeedParcel dataset. This includes integrating SpeedParcel's business capability support matrix into the knowledge graph, which is in the format of an Excel file. To prepare this data for the knowledge graph, a parser will be written that is able to transform the architecture diagrams into nodes and relationships.

A further objective is to improve the querying method. Alternative approaches are to be tried out during this phase in order to validate whether the embedding method is the right choice or if alternative methods are more viable in this case. Alternative approaches are to be researched and tried out as deemed appropriate by the researcher. On top of this, alternative LLM models are to be tested through similar research.

% Action Taken
\section{Action Taken}
\label{sec:cycle2_actionTaken}
The transformation of SpeedParcel's capability support matrix was approached by developing a customized parser, specifically written for the Excel file containing the capability support matrix. This parser was developed by prompting an LLM with what the structure of the data looks like and by creating and iteratively finetuning a Python script which reads in the file, breaks it down based off of the predefined data structure, and saves the created nodes and edges into the Neo4j database. This approach required iteratively testing the data added to the knowledge graph in order to ensure that it resembled the original data structure. After several rounds of fine tuning, the capability support matrix could be read into the knowledge graph.

In order to solve the previous challenge of overreliance on an LLM when developing, it was decided by the researcher that the backend will be rewritten in Node.js\footnote{Node.js: \url{https://nodejs.org/en}}, as the researcher is more versed in JavaScript backends than in Python. This rewrite did not change the core logic of the application and still allowed for typical client-server HTTP communication between the frontend and backend. However, this step enables a more maintainable codebase as well as more transparent and scalable future development.

During the backend refactoring, the idea of a Model Context Protocol (MCP) server came up during development. Because Neo4j offers MCP support\footnote{Neo4j MCP: \url{https://neo4j.com/docs/mcp/current/}} via an APOC plugin, creating the MCP server and connecting it to the Node.js backend simply required executing predefined functions. This brought the advantage of a standardized interface between the application and the knowledge graph.

However, the inappropriate querying method remained a challenge that needed to be overcome. An approach was tried out in which the user's input was used as context in a prompt towards an LLM which is tasked with generating Cyphers to retrieve the information that the user is requesting. Rather than creating an embedding based off of the user's input, the user's input is now being used as context for an LLM to generate the necessary queries for the knowledge graph. This text-to-Cypher approach requires that a system prompt be passed as context to the LLM, supplementing the user's input with additional information and instructions of what the LLM is being tasked with. For example, the first version of the system prompt gives the LLM information about the knowledge graph's structure, including the structure of the textbook and the capability support matrix. It is also given basic instructions for the task of creating Cyphers based off of the user input for the specified knowledge graph structure. The LLM is then able to use this system prompt alongside the user's input in order to generate appropriate Cyphers to extract the necessary data from the knowledge graph.

Implementing the MCP server along with the text-to-Cypher approach helps achieve a higher quality in the retrieved answers, as the user's prompt is now being transformed into custom Cyphers with the help of the system prompt. This allows the Cyphers being used to query the database instead of filling out the parameters of a hard-coded Cypher as before. The MCP server brings the advantage of executing these Cyphers along a standardized interface, rather than by connecting the backend directly to the database and executing the Cyphers directly on the database.

Lastly, the Qwen model was replaced with the latest GPT-5.2\footnote{\url{Open AI GPT-5.2: https://developers.openai.com/api/docs/models/gpt-5.2}} model from Open AI. This change required replacing all calls to the previous model with calls to Open AI's API. The advantage of using such a mainstream model is the increased speed and quality in generated answers as well as the improved development support due to more documentation and a higher expected lifetime of the model. However, this comes with over head costs per request being made toward Open AI.

% Evaluation
\section{Evaluation}
\label{sec:cycle2_evaluation}
The matured prototype was again evaluated qualitatively through a demonstration by the researcher and open discussions between all three stakeholders. Both advisors expressed strong praise in the approach and assured that the chatbot was being developed in the correct direction in order to achieve the end goal of allowing enterprise architects to interact with a proprietary knowledge graph via natural language. The chatbot was able to answer conceptual questions within the realm of EAM with improved accuracy and consistency. It was also able to answer questions regarding the parsed capability support matrix. The results with the new text-to-Cypher approach along with the MCP server enable more reliable and higher quality answers. The new LLM being used also generates answers within seconds, rather than minutes, and does so with a higher quality natural language output.

A prerequisite for the Cypher generation to work, however, is that the schema of the knowledge graph is known. At this stage, the schema is hard-coded into the backend. This will require adapting the system context to further datasets that will be added to the knowledge graph. This hard-coded mechanism is noted as a potential bottleneck when attempting to scale the application to a more heterogeneous set of imported architecture diagrams.

It was during this evaluation that the practitioners suggested that Action Research be applied as the research method for the explorative development process. This was deemed fitting because the approach so far has by design strongly resembled that of Action Research. Adapting to formal Action Research simply meant introducing academic conditions into the approach, such as documenting discussions with the goal of reproducing the efforts within this thesis. Committing to Action Research also meant that the focus can continue to be laid on developing the prototype, as opposed to spending time and effort laying out how the system may be tested. This allowed for a longer development phase within the timeframe of the thesis, enabling a more well rounded prototype to be strived for as the finished product. Applying Action Research also means that the practitioners not only assume the responsibility of giving feedback on the current implementation, but are also responsible for generating ideas on how to move forward.

During the discussions, it was regularly mentioned that enterprise architects work on architectural artifacts within the application Archi. This means that moving forward, a requirement of the system is to be able to import and interpret the enterprise architecture diagrams exported from Archi.

It was also during these discussions where a deadline for the prototype development was agreed upon. Alongside the deadline, a final review was discussed where all three stakeholders will be testing and evaluating the system together. This also means that the prototype must be prepared for such a final review. The use case of a final review requires that a local deployment be setup in order to allow the other practitioners to have the prototype running on their local computers. It was agreed up to use an approach with Docker\footnote{\url{https://www.docker.com/}}, as this allows each system component to be added to an encapsulated container, enabling an operating system agnostic execution method of the prototype.

Furthermore, the practitioners and researcher identified three key categories of user questions. The first category identified conceptual questions related to enterprise architecture principles, concepts, and best practices found in textbooks. The second category contains descriptive questions targeting concrete architectural elements and relationships. The third and final category contains integrative questions combining conceptual knowledge with specific architectural contexts. Examples of these categories can be found in the Appendix's chapter \ref{appendix:ch:example_questions}. This overlaps with the categories identified in section \ref{sec:intro:relatedWork} by Zhao et al. (2024) \cite{zhao2024retrieval}, where explicit facts map to the identified category 1, implicit facts map to category 2, and the interpretable rationales and hidden rationales map to category 3. This categorization of potential questions will help structure the final review.

The practitioners also expressed and envisioned practical use cases for the finished prototype in which an EAM project plan is developed with the support of the developed chatbot. For example, the chatbot may support in identifying which applications are redundant in an enterprise's architecture. Subsequent analysis of the business capabilities influenced by this change can be supported by the chatbot as well. This confirms that the prototype in its current state has the potential to be morphed into a practical tool used in real world settings, extending beyond the scope of only being applied for research purposes.

% Learning
\section{Learning}
\label{sec:cycle2_learning}
As the prototype is matures, more learnings can be pulled from each phase.

Firstly, the hard-coded query method that was previously implemented was proven to be insufficient for dynamic interaction with the evolving graph structure. The new text-to-Cypher approach confirms this as the information being extracted from the knowledge graph is more accurate in regard to answering the user's input.

At the same time, the switch to a mainstream LLM allowed for faster response generation as well as higher quality natural language outputs.  

The MCP implementation allows the entire system to be more dynamic, independent of the data in the knowledge graph.


Beyond the scope of this thesis, a broader vision by the advisors emerged in which the prototype could support project planning by assessing impacts on application interfaces, systems, and stakeholders. The co-advisor noted that the developments achieved thus far provide a starting point for exploring such directions in the subsequent cycles as well as in future work beyond the scope of this research.


