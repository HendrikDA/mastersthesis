\chapter{Cycle 2}
\label{ch:cycle2}
The second cycle builds upon the foundation laid during the first and is the shortest cycle of the four. However, the second cycle allowed extending the proof of concept from the first cycle and makes clear what the path toward a finished prototype version might look like.

% Diagnosis
\section{Diagnosis}
\label{sec:cycle2_diagnosis}
Following the initial feasibility assessment, the next identified challenge concerned extending the knowledge graph with more information and positioning the prototype as a useful tool within the realm of enterprise architecture management. This means adding the first bits of architectural data to the knowledge graph as well as improving the shortcomings of the querying method and LLM runtime.

One design consideration involved separating textbook-based domain\linebreak knowledge from enterprise architecture data into two distinct databases. After discussing this, a single integrated database was chosen to enable direct relationships between conceptual textbook knowledge and architectural data, with the expectation of improved contextual reasoning.

The challenge imposed by lack of real-world data to test the system was also identified. Potential test datasets for further development and evaluation were discussed in order to bridge the gap before real-world data could be applied. To aid development, it was agreed upon that the data from a completed university assignment from the Frankfurt University of Applied Sciences be used, consisting of an application landscape, business capability map, business capability support matrix, business object model, and cross-application data-flow diagram for a fictitious company named \textit{SpeedParcel}. This SpeedParcel dataset will be used throughout the next cycles.

% Action Planning
\section{Action Planning}
\label{sec:cycle2_planning}
The objective of the second cycle is to extend the knowledge graph with additional data from the SpeedParcel dataset. This includes integrating SpeedParcel's business capability support matrix into the knowledge graph, which is found in an Excel file. To prepare this data for the knowledge graph, a parser will be written that is able to transform the architecture diagrams into nodes and relationships.

A further objective is to improve the querying method. Alternative approaches are to be tried out during this phase in order to validate whether the embedding method is the right choice or if alternative methods are more viable in this case. On top of this, alternative LLM models are to be tested.

% Action Taken
\section{Action Taken}
\label{sec:cycle2_actionTaken}
The transformation of SpeedParcel's capability support matrix was approached by developing a customized parser, specifically written for this exact dataset. This parser was developed by explaining to an LLM what the structure of the data looks like and creating a Python script which reads in the file, breaks it down based off of the predefined data structure, and saves the created nodes and edges into the Neo4j database. This approach required iteratively testing the data added to the knowledge graph in order to ensure that it resembled the original data structure. After several rounds of fine tuning, the capability support matrix could be read into the knowledge graph.

In order to solve the previous challenge of overreliance on an LLM when developing, it was decided that the backend will be rewritten in Node.js\footnote{Node.js: \url{https://nodejs.org/en}}, as the researcher is more versed in JavaScript backends than in Python. This rewrite did not change the core logic of the application and still allowed for typical client-server HTTP communication between the frontend and backend. However, this step will enable a more maintainable codebase as well as more transparent and scalable future development.

During the backend refactoring, the idea of a Model Context Protocol (MCP) server came up during development. Because Neo4j offers MCP\footnote{Neo4j MCP: \url{https://neo4j.com/docs/mcp/current/}} support via an APOC plugin, creating the MCP server and connecting it to the Node.js backend was able to be completed quickly. This brought the advantage of a standardized interface between the application and the knowlede-graph.

However, the poor querying method remained a challenge that needed to be overcome. An approach was tried out in which the user's prompt was used as context in a prompt towards an LLM which is tasked with generating Cyphers to retrieve the information that the user is requesting. Rather than creating an embedding based off of the user's input, the user's input is now being used to allow an LLM to generate the necessary queries for the knowledge graph. This text-to-Cypher approach requires that a system prompt be passed as context to the LLM, supplementing the user's input with additional information of what the LLM is being tasked with. For example, the first version of the system prompt gives the LLM information about the knowledge graph's structure, including the structure of the textbook and the capability support matrix. The LLM is then able to use this system prompt alongside the user's input in order to generate Cypher.

Implementing the MCP server along with the text-to-Cypher approach helps achieve a higher quality in the retrieved answers, as the user's prompt is now being transformed into custom Cyphers with the help of the system prompt. This allows the Cyphers being used to query the database instead of filling out the parameters of a hard-coded Cypher as before.

Lastly, the Qwen model was replaced with the latest GPT-5.2\footnote{\url{Open AI GPT-5.2: https://developers.openai.com/api/docs/models/gpt-5.2}} model from Open AI. This simple required replacing all calls to the previous model with calls to Open AI's API. The advantage of using such a mainstream model is the increased speed and quality in generated answers as well as the improved development support due to more documentation and a higher expected lifetime of the model.


% Evaluation
\section{Evaluation}
\label{sec:cycle2_evaluation}
The extended prototype was again evaluated qualitatively through a demonstration by the researcher and open discussions between all three stakeholders. Both advisors expressed strong interest in the approach and assured that the chatbot was being developed in the correct direction in order to achieve the end goal of allowing enterprise architects to interact with the knowledge graph via natural language. The chatbot was able to answer conceptual questions within the realm of EAM with improved accuracy and consistency. It was also able to answer questions regarding the parsed capability support matrix. The results with the new text-to-Cypher approach along with the MCP server enabled more reliable and higher quality answers. The new LLM being used also generated answers within seconds, rather than minutes, and does so with a higher quality natural language output.

A prerequisite for the Cypher generation to work, however, was that the schema of the knowledge graph was known. At this stage, the schema was hard-coded into the frontend. This will require adapting the system context to further datasets that will be added to the knowledge graph.

It was during this evaluation that the practitioners suggested that Action Research be applied as the research method for the explorative development process. This was deemed fitting because the approach so far by design strongly resembled that of Action Research. Adapting to formal Action Research simply meant introducing academic conditions into the approach. Committing to Action Research also meant that the focus can continue to be laid on developing the prototype, as opposed to spending time laying out how the system may be tested. This allowed for a longer development phase within the timeframe of the thesis enabling a more mature prototype to be strived for. This also meant that the practitioners not assumed the responsibility of giving feedback on the current implementation, but were also responsible for generating ideas on how to move forward.

During the discussions, it was regularly mentioned that enterprise architects work on architectural artifacts within the application Archi. This means that moving forward, it is a requirement of the system to be able to import and interpret the enterprise architecture diagrams exported from Archi.

It was also during these discussions where a deadline for the prototype development was agreed upon. Alongside the deadline, a final review was agreed upon where all three stakeholders will be testing and evaluating the system together. This also means that the prototype must be prepared for such a final review. This use case requires that a local deployment be setup in order to allow the other practitioners to have the prototype running on their local computers. It was agreed up to use an approach with Docker\footnote{\url{https://www.docker.com/}}, as this allowed each system component to be added to an encapsulated container, enabling an operating system agnostic execution method of the prototype.

Furthermore, the practitioners and researcher identified three key categories of user questions, similar to the ones described in section \ref{sec:intro:relatedWork} by Zhao et al. (2024) \cite{zhao2024retrieval}. The first category being conceptual questions related to enterprise architecture principles, concepts, and best practices found in textbooks. The second category being descriptive questions targeting concrete architectural elements and relationships. The third category being integrative questions combining conceptual knowledge with specific architectural contexts. Examples of these categories can be found in the Appendix's chapter \ref{appendix:ch:example_questions}.


% Learning
\section{Learning}
\label{sec:cycle2_learning}
As the prototype is starting to mature, more learnings are being pulled from each phase. Firstly, the hard-coded query method that was previously implemented was proven to be insufficient for dynamic interaction with the evolving graph structure. The new text-to-Cypher approach confirms this as the information being extracted from the knowledge graph is more accurate in regard to answering the user's input. At the same time, the switch to a mainstream LLM allowed for faster response generation as well as higher quality natural language outputs.  

The MCP implementation allows the entire system to be more dynamic, independent of the data in the knowledge graph.


Beyond the scope of this thesis, a broader vision by the advisors emerged in which the prototype could support project planning by assessing impacts on application interfaces, systems, and stakeholders. The co-advisor noted that the developments achieved thus far provide a starting point for exploring such directions in future work.


