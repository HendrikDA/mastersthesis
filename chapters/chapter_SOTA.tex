\chapter{Current State of the Art}
\label{ch:SOTA}
In order to understand how this research fits into the current state of the art, it is beneficial to go over current research and implementations. This helps to scientifically position the research at hand and justifies its implementation methods. This chapter describes how research on the current state of the art was conducted and summarizes the most relevant papers and projects found.

%
% PRISMA Framework
%
\section{PRISMA Framework}
\label{sec:prisma}
The importance of documenting the research process in order to demonstrate the exhaustiveness of the research conducted has been emphasized in prior work. A robust literature research comprises querying databases for relevant literature using keywords as well as backward and forward searches based on these findings. Because of the plethora of literature on the subject matter, systematically including and excluding existing research has to be made as transparent as possible. Describing which existing research was included and excluded is vital for the credibility of a literature analysis. \cite{brocke2009reconstructing, webster2002analyzing}

To support in this, the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework was applied. The goal of PRISMA is to show why the review was done, how studies were identified and considered, and what was found. \cite{page2021prisma}


Literature was primarily identified using the Google Scholar search engine. The search process was conducted by defining relevant keywords and searching for these in the search engine. Forward and backward citation tracking was also applied to the extracted sources. Search terms were iteratively improved and clustered around the main themes of this thesis. Search terms included EAM, LLMs, RAG, and Conversational Agents. In addition to manual search methods, a small number of relevant sources were found through prompted searching via ChatGPT \cite{openai_chatgpt_2023}. A flowchart of the applied PRISMA framework can be seen in figure \ref{fig:prisma}.

Setting up criteria to filter out excessive literature is necessary in order to only be left with the relevant pieces. Academic, peer-reviewed sources were the main filtering criteria. This was to ensure the academic integrity of the research process. The research's focus was then laid on the categories defined by the keyword search terms. The sources found were required to provide value to this thesis. Because this work is very practically oriented, more design and implementation oriented approaches were analyzed, as opposed to meta analyses. A further crucial filtering criteria applied was the publishing date. This is especially relevant for research on LLMs and conversational agents, as LLMs such as ChatGPT only adopted widespread use in late 2022 \cite{buchmann2024large}, meaning research before then is less likely to be applicable to this research. Meta data of a published paper such as the amount of times it has been cited by further papers may also be an indication of how well grounded the source within its domain. For example, a paper cited by thousands of further papers is likely to be a key paper within its domain. A paper cited by little to no further papers is to be critically considered. Internet sources were kept to a bare minimum.

% Figure for PRISMA Framework
\begin{figure}[H]
\centering
\makebox[\textwidth][c]{%
  \includegraphics[width=1\textwidth]{images/PRISMA}
}
\caption{A flowchart of the applied PRISMA framework depicting how many sources were initially found with each search method and how many were left for the final current state of the art analysis. The framework is applied as seen in \cite{mishra2023prisma}.}
\label{fig:prisma}
\end{figure}

Applying the above filters, papers found were initially skimmed in order to assess if it may be relevant to this research. This meant going over the papers' abstracts and key points, but not reading every detail yet. This allows papers that do not support the goal of this thesis to be filtered out quickly. Afterwards, the remaining papers were read more thoroughly.


By applying the PRISMA framework as mentioned above, it was possible to reduce the amount of sources from an initial count of x sources down to y sources after the first step and down to the final z sources after the final step.	


% You don’t need to retell everything — the goal is to extract what is relevant to your thesis. A good mini-summary (2–5 sentences) should cover:
%Context: What problem or domain does it deal with?
%Approach: What methods, models, or tools does it use?
%Findings / Contributions: What are the main results?
%Relevance for you: Why does this matter for your thesis? (e.g., “demonstrates limitations of plain RAG approaches — motivates KG integration”)
% Template on how to summarize a paper: "Authors (Year) investigate X using Y. They found Z. For my thesis, this shows A / highlights gap B."

%
% Related Work and Research Landscape
%
\section{Related Work and Research Landscape}
\label{sec:intro:relatedWork}
This section synthesizes and discusses the relevant literature and projects found through the previously described PRISMA-based research process. The selected works are semantically grouped and allow the reader to understand how the research at hands fits within the current research landscape.

%%%%%%%%%% -------------------------------------------------------------------------------------
% Jung, Fraunholz 2021: Masterclass Enterprise Architecture Management
% Key points: Supplementing an LLM with RAG
% Todo: start with a high-level explanation of EAM and what the common challenges are here.
theories, digital twin efforts, EA tool landscapeStandards or frameworks (e.g., TOGAF, ArchiMate, IATA ONE Record, LeanIX).
Theoretical foundations (auch auf prozessmanagment eingehen, wie der aktuelle Prozess aussieht, wenn die Landschaft geändert werden soll)
Current tools and methods
Research prototypes in EA

Authors Jung and Fraunholz 2021 \cite{jung2021masterclass} lay foundational work from which many EAM concepts can be derived.
%%%%%%%%%% -------------------------------------------------------------------------------------

This high-level overview of EAM show that some of the challenges faced by enterprise architects may be supported with the advancements of modern AI. Tasks such as todo (maybe that junior architects need a lot of time to understand the data and retrieve information for architectures?) are predestined to be supported by LLMs.

However, LLMs are notorious for hallucinating, meaning that incorrect information is spewed while looking as though it could be factually correct. This is a major downside when interacting with an LLM, especially if expert knowledge in a specific domain is required. An LLM is trained on a large corpus of data and generates its answers through probability calculations. It is thus not concerned with whether or not the returned information is factually correct, but rather only if the text is human-like. \cite{hicks2024chatgpt}

%%%%%%%%%% -------------------------------------------------------------------------------------
% Zhao et al. 2024: Retrieval augmented generation (rag) and beyond: A comprehensive survey on how to make your llms use external data more wisely
% Key points: Supplementing an LLM with RAG and categorizing levels of complexity
Zhao et al. (2024) \cite{zhao2024retrieval} investigate how to overcome this issue by supplementing an LLM with a RAG-based solution. Augmenting the context given to an LLM comes with the advantage that the data is up to date, aligned with domain expert knowledge, reduces hallucination, and the data supplied in the context explains the answers generated.

A key challenge mentioned by the authors when developing an RAG-based LLM system is enabling the LLM to comprehend varied forms of data necessary within the applied domain. In the case of the research at hand, this means that a central challenge when developing the RAG-based LLM system will be to structure and query the data in such a way that the LLM is given the right context to respond to the user with factual correctness. It is also mentioned that making a system transparent helps in increasing trust and reliability in the system.

To tackle this challenge, the authors identify four levels of complexity when it comes to information retrieval. The levels are as follows and range from level 1, being less complex queries, to level 4, which require complex information retrieval operations.

% Four levels of query-complexity
\begin{enumerate}[label=Level \arabic*, leftmargin=*]
	\item Explicit Facts: Require no reasoning and are facts found directly in the data.
	\item Implicit Facts: May require common reasoning or logical deductions based on the data.
	\item Interpretable Rationales: Require a grasp of the factual content as well as domain-specific knowledge which is integral to the data's context.
	\item Hidden Rationales: Require that more information than that which is being asked is leveraged to generate the answer.
\end{enumerate}

While queries in level 1 and level 2 are rather straightforward and only require the system to retrieve the relevant facts, queries within level 3 and level 4 require that the system has the capacity to learn and apply the rationales that go above and beyond the data itself. The challenge of query tasks which require rationale can be overcome through in-context learning. The authors describe this as the ability of an LLM to infer hidden rationales by being given relevant examples within the context of the prompt.

These categories will be relevant again later in chapter \ref{ch:method} as these are also applicable within the domain of enterprise architecture management. Each question posed to the system within this research can be broken down into one of these four categories.

However, this does not answer the question of how information from a knowledge graph can be retrieved. This is especially challenging as each question by a user contains a certain level of complexity which needs to be covered by the generated query. How can this challenge be overcome?

%%%%%%%%%% -------------------------------------------------------------------------------------


%%%%%%%%%% -------------------------------------------------------------------------------------
% Wan et al. 2025: Prompting large language models based on semantic schema for text-to-Cypher transformation towards domain Q\&A
% Key points: Text-to-cypher
Wan et al. (2025) \cite{wan2025prompting} investigate how LLMs can effectively be employed in order to generate Cypher queries and retrieve domain-specific information from a knowledge graph in order to supplement LLM-generated answers. The authors propose a text-to-cypher approach that leverages a lightweight, semantic schema derived from the domain ontology. By injecting the relevant parts of this schema at query runtime, the LLM can be guided toward generating syntactically correct and semantically aligned Cypher queries without requiring model retraining and without overloading the LLM's context. The injected schema context enables the LLM to generate domain-aligned answers grounded in the structure of the knowledge-graph.

For this thesis, this illustrates that a text-to-Cypher approach offers key advantages, such as aligning an LLM with domain-specific information and enabling domain-aligned reasoning without retraining. Additionally, this allows an LLM to remain up to date with a changing knowledge graph by dynamically incorporating the updated schema information at runtime. By exposing the knowledge graph through natural language inputs, the proposed approach lowers the barrier for non-technical users to interact with and extract information from the knowledge graph, allowing for a democratization of the information within it. The research by Wan et al. further reinforces the suitability of a graph-based solution for the research at hand, as enterprise architecture involves highly interconnected data and requires reasoning across various entities and their relationships. 
%%%%%%%%%% -------------------------------------------------------------------------------------

Connecting the research conducted by Zhao et al. (2024) and Wan et al. (2025), it becomes clear that a RAG-based LLM system must be able to categorize the complexity of a user's request and build the queries accordingly. While Zhao et al. (2024) provide a high-level framework for understanding the complexity of the task given by the user, their work remains agnostic to concrete query-generation techniques required to interact with the knowledge graph. This gap is addressed by Wan et al. (2025) through their text-to-cypher approach.

This now begs the question of how such a RAG-based LLM system is able to retrieve the relevant information from the knowledge graph in order to answer the user's prompt with factual correctness.

%%%%%%%%%% -------------------------------------------------------------------------------------
% Chen et al. 2025: GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models
% Key points: Schema
Chen et al. (2025) \cite{chen2025gril} propose a graph-retrieval approach that is tightly integrated with an LLM by allowing it to iteratively retrieve and refine relevant subgraphs from a knowledge graph for downstream reasoning. This works by identifying seed entities within the graph based on the user's prompt. A graph retriever then iteratively expands from these seed entities along promising relations while pruning irrelevant nodes and edges. The extracted relevant subgraphs are then given to the LLM as context for reasoning. This multi-hop retrieval strategy allows the system to retrieve complex relational information that is necessary to answer more complex user queries.

In contrast to this multi-hop approach, simpler approaches rely on the LLM-as-retriever paradigm. In such approaches, the LLM performs a one-time extraction based on what the LLM thinks is relevant within the graph. Chen et al. explicitly criticize this LLM-as-retriever strategy. They argue that this does not allow the subgraphs and its intricacies to be fully exploited. Moreover, it requires that multiple LLM calls be made to explore different parts of the graph, leading to complex queries and scalability issues when applied to large knowledge graphs.

%%%%%%%%%% -------------------------------------------------------------------------------------

So far it has only been discussed how to retrieve information from a knowledge graph. However, it has not yet been discussed how to persist information within a graph database. Preparing and storing data in a graph representation comes with challenges of its own. The applicable method to break data down into nodes and edges depends on the source data. For example, converting a textbook, which is semi-structured, will require a different approach compared to converting an existing application landscape, which is structured data.


%%%%%%%%%% -------------------------------------------------------------------------------------
% author jahr: titel
% Key points: Umwandlung von Daten in eine GraphDB


%%%%%%%%%% -------------------------------------------------------------------------------------



Taking a step back from the technical details of a RAG-based implementation, it fair to ask why it is even necessary to digitalize the work that can be done by expert enterprise architects.

This paper covers how ai tools are more scalable than manual expertise analysis of things. The source is highly relevant. Look at the summary in notebookLM. 05.10.25 \cite{gu2024survey}

This paper \cite{evaluationFrameworkLLMs} gives a standardized method and framework for evaluating conversational AI agents.


The next section takes a step away from academic papers and delves into similar projects and prototypes.


%
% Related Projects
%
\section{Related Projects}
\label{sec:intro:projects}
Because the research at hand deals with the development of a prototypical RAG-based LLM chatbot, it is worthwhile to have a look at similar projects on the market. This section also takes a look at common challenges faced by such chatbots.


This paper gives an overview on how to control the dialog sequence and also notes 4 types of dialog options for chatbots in the related works section: \cite{li2025chatsopsopguidedmctsplanning}.


%
% Comparable Projects and Prototypes
%
Proof-of-concepts, research prototypes, industry whitepapers, GitHub projects.

Tools like ChatEA, LeanIX AI features, or Microsoft Copilot integrations in architecture/governance.

A prototypical graph-based RAG approach for text-summarization has been created by Microsoft: y\cite{MsGraphRAGPrototype}. The accompanying paper is here: \cite{MsGraphRAGPaper}

Dragon1 needs to be detailed here!






% Closing paragraph that describes the current gap in the research landscape and a high-level view of how i close the gap.
The reviewed literature demonstrates that recent advances in LLM-based text-to-cypher approaches xyz... However, existing approaches xyz (what do they lack?). This thesis builds upon these findings by proposing a graph-based, schema-aware approach tailored to enterprise architects and enterprise architecture data, aiming to combine the flexibility of LLMs with the required domain knowledge of enterprise architecture.



