\chapter{Current State of the Art}
\label{ch:SOTA}
In order to understand how this research fits into the current state of the art, it is beneficial to go over current research and implementations. This helps to scientifically position the research at hand and justifies its implementation methods. This chapter describes how research on the current state of the art was conducted and summarizes the most relevant papers and projects found.

%
% Research Method
%
\section{Literature Research Method}
\label{sec:intro:research}

The importance of documenting the research process in order to demonstrate the exhaustiveness of the research conducted has been emphasized in prior work. A robust literature research comprises querying databases for relevant literature using keywords as well as backward and forward searches based on these findings. Because of the plethora of literature on the subject matter, systematically including and excluding existing research has to be made as transparent as possible. Describing which existing research was included and excluded is vital for the credibility of a literature analysis. \cite{brocke2009reconstructing, webster2002analyzing}

To support in this, the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) framework was applied. The goal of PRISMA is to show why the review was done, how studies were identified and considered, and what was found. \cite{page2021prisma}
% todo
\subsection{PRISMA}
\label{sec:prisma}
Literature was primarily identified using the Google Scholar search engine. The search process was conducted by defining relevant keywords and searching for these in the search engine. Forward and backward citation tracking was also applied to the extracted sources. Search terms were iteratively improved and clustered around the main themes of this thesis. Search terms included EAM, LLMs, RAG, and Conversational Agents. In addition to manual search methods, a small number of relevant sources were found through prompted searching via ChatGPT \cite{openai_chatgpt_2023}.

Setting up criteria to filter out excessive literature is necessary in order to only be left with the relevant pieces. Academic, peer-reviewed sources were the main filtering criteria. This was to ensure the academic integrity of the research process. The research's focus was then laid on the categories defined by the keyword search terms. The sources found were required to provide value to this thesis. Because this work is very practically oriented, more design and implementation oriented approaches were analyzed, as opposed to meta analyses. A further crucial filtering criteria applied was the publishing date. This is especially relevant for research on LLMs and conversational agents, as LLMs such as ChatGPT only adopted widespread use in late 2022 \cite{buchmann2024large}, meaning research before then is less likely to be applicable. Meta data of a published paper such as the amount of times the it has been cited by further papers may also be an indication of how well grounded the source within its domain. For example, a paper cited by thousands of further papers  Internet sources were kept to a bare minimum.



Define the scope (what fields you looked at, which databases, what keywords). Define the research method and how you narrowed it down from x sources to y sources.i
Remember: Scientific positioning and comparison

To do: declare how chatgpt was used to help research. the apa style documentation notes how to cite and describe it. use this source \cite{openai_chatgpt_2023}

%PRISMA-lite anwenden: search strings definieren (zB "EAM", "LLM"), durchsuchte datenbanken aufzählen (zB Google Scholar), wie viele Treffer ich gefunden habe und wie viele ich davon rausgefiltert habe. Prisma-flowchart zeigen? zB von 150 hits -> 30 nach dem ersten screening -> 15 am Ende übrig geblieben

% You don’t need to retell everything — the goal is to extract what is relevant to your thesis. A good mini-summary (2–5 sentences) should cover:
%Context: What problem or domain does it deal with?
%Approach: What methods, models, or tools does it use?
%Findings / Contributions: What are the main results?
%Relevance for you: Why does this matter for your thesis? (e.g., “demonstrates limitations of plain RAG approaches — motivates KG integration”)
% Template on how to summarize a paper: "Authors (Year) investigate X using Y. They found Z. For my thesis, this shows A / highlights gap B."


%
% Enterprise Architecture Management
%
\section{Enterprise Architecture Management}
\label{sec:intro:eam}
theories, digital twin efforts, EA tool landscapeStandards or frameworks (e.g., TOGAF, ArchiMate, IATA ONE Record, LeanIX).
Theoretical foundations (auch auf prozessmanagment eingehen, wie der aktuelle Prozess aussieht, wenn die Landschaft geändert werden soll)
Current tools and methods
Research prototypes in EA

Authors Jung and Fraunholz 2021 \cite{jung2021masterclass} lay foundational work from which many EAM concepts can be derived.

%
% LLMs, Conversational Agents, and RAG
%
\section{Large Language Models and Retrieval-Augemented Generation}
\label{sec:intro:llmandrag}
strengths, hallucination issues, graph-RAG enhancements
Theoretical foundations
Current tools and methods

This paper covers how ai tools are more scalable than manual expertise analysis of things. The source is highly relevant. Look at the summary in notebookLM. 05.10.25 \cite{gu2024survey}

This 2025 paper has ideas on how changing knowledge graphs (e.g. through updates) can be handled \cite{temporalGraphRag}. It looks at temporal data and how to handle it. This might be relevant since addressing how a changing application landscape can be handled will probably be a challenge.

This paper gives an overview on how to control the dialog sequence and also notes 4 types of dialog options for chatbots in the related works section: \cite{li2025chatsopsopguidedmctsplanning}.

This paper \cite{Zhai_2025} covers how a chatbot can support in task-planning and output generation. Might be helpful in understanding how my chatbot can tell the EA how to conduct changes in the application landscape.

This paper \cite{deng2023promptingevaluatinglargelanguage} states how proactive dialogue systems work and can be improved. It goes into detail on 3 types of dialogue systems: clarification, target-guided, and non-collaborative dialogues. All 3 of these have a certain relevance for the EAM Chatbot.

RAG: Geh darauf ein, was es für unterschiedliche Chunking methoden gibt, wie man ein Buch runterbringt, und was das alles für vor und nachteile hat. auch welche tools es gibt (neo4j) ist wichtig.

Rag: Basic overview of RAG \cite{zhao2026retrieval}

RAG: \cite{lewis2020retrieval}

%
% Text to Cypher Generation
%
\section{Text-to-Cypher Generation}
\label{sec:text_to_cypher}
Text-to-cypher: this source is OP \cite{wan2025prompting}. it goes into detail on why text to cypher is beneficial, how it is done, how to prompt the system, and how they used a 3 step system to preprocess the text, map the preprocessed text to the entities in the db, generate the cypher based on all previous information and context.

%
% Comparable Projects and Prototypes
%
\section{Comparable Projects and Prototypes}
\label{sec:intro:prototypes}
Proof-of-concepts, research prototypes, industry whitepapers, GitHub projects.

Tools like ChatEA, LeanIX AI features, or Microsoft Copilot integrations in architecture/governance.

A prototypical graph-based RAG approach for text-summarization has been created by Microsoft: y\cite{MsGraphRAGPrototype}. The accompanying paper is here: \cite{MsGraphRAGPaper}

Dragon1 needs to be detailed here!

%
% Evaluations and Limitations
%
\section{Evaluations and Limitations}
\label{sec:intro:limitations}
Studies analyzing strengths/weaknesses of RAG, embedding quality, hallucination mitigation.

Papers about user interaction with EA tools, chatbot evaluation frameworks, usability challenges

This paper \cite{evaluationFrameworkLLMs} gives a standardized method and framework for evaluating coversational AI agents.

This paper \cite{liu2023agentbench} proposes a benchmark for open-ended multi-turn conversational agents. I think this paper focuses more on evaluating agents and comparing their results, but maybe i can copy their evaluation methods and benchmarks?







