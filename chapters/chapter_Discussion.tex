\chapter{Results and Discussion}
\label{ch:Discussion}
This chapter is meant to evaluate the implementation. what went well. what didn't go so well. etc.


note that the custom parsers for the textbook information and the first parts of the speedparcel data worked well during the proof of concept phase, but the switch to the xml parser was needed to fulfil the requirements.

possible improvements:
\begin{itemize}
	\item The schema call before every prompt is costly for large schemas (i assume as of 02.01.26). This source from wan \cite{wan2025prompting} confirms this - giving the complete schema information can overload the context. This limitation with character limits and overly large contexts is also mentioned in this paper in the conclusion's limiations: \cite{mihindukulasooriya2023text2kgbench}
	\item Does the APOC XML parser create noise in the database? you know how the XML files have view-data in it on how to display it in archi? we're not leaving that out. does that get saved into the database?
	\item The LLM often doesn't know which node type to look in. E.g. if in archi a landscape was created with information in a "BusinessFunction" node and this is queried in Masuta, then the LLM often doesn't know that the information it should look for is in the business function node type. it searches elsewhere, doesn't find anything, and returns 0 results. This not knowing what node type to look in without supplying the context in the prompt is a necessary improvement.
	\item Document parsing with other tools may have helped speed up the process in the beginning, allowing for more well-developed features later in the project. For example doc ling https://www.docling.ai/
\end{itemize}


Den Kreis schließen zur ursprünglichen Diskussion, ob man wirklich eine graph datenbank braucht. Rainer am 09.01 hat erwähnt, dass es denkbar ist, dass man im kleineren Scope gar keine DB braucht, sondern dass die KI sich den Kontext selber irgendwie blackbox artig aufbaut. Die graph db bräuchte man dann nur im falle einer gewissen größe. Ein Skalierungsproblem. dafür ist mein system vollständig transparent. ohne vendor lock in. 

Die wichtigsten Punkte aus dem Workshop:
\begin{itemize}
	\item Kontext ist alles bei masuta. die selbe frage mit minimalen anpassungen erzeugt bessere ergebnisse. wenn man fragen mit gewissem kontext stellt (prompting mit fachwörtern), dann kommt er besser / schlechter mit den anfragen klar. zB wenn man die hochschul-prüfungsamt-daten reinlädt und als person vom prüfungsamt abfragt, dann bekommt man gute antworten weil diese person idR. fachwörter benutzt. aber, wenn ein student die abfrage macht, dann kommt schlechteres bei rum, weil keine fachwörter bei rumkommen. Es ist aber klar zu sehen, dass Masuta gewisse Infos von sich aus mitbringt. zB bei einer Frage zu HIS hat Masuta das QIS system erwähnt, dass es das auch gibt. QIS taucht aber nirgendwo in den daten auf. das heißt chatgpt hat sich das aus eigenem LLM gezogen und zurückgegeben.
	\item wie groß ist das risiko bei der weiterentwicklung? wenn masuta etwas nicht kann, wie leicht ist es, das system anzupassen?
	\item festhalten: mit RAG kriegt man expertenwissen in das system rein. das kann Rovo zB nicht - er wird niemals ein EAM experte sein über dem, worauf das LLM trainiert ist, hinaus
	\item fehlermeldungen, zB dass masuta immer wieder versucht hat, informationen in die DB zu schreiben (was nicht geht weil es read only ist und damit viele fehlermeldungen geworden hat). das ist ein gotcha, worauf man achten muss.
	\item USP: die transparenz von masuta. man kann alles genau nachschlagen, wie er die antwort generiert hat, indem man sich die cypher anschaut. Referenzier nochmal das paper im SOTA \cite{zhao2024retrieval} wo genau das als vorteil beschrieben wird.
	\item arbeitsweise mit masuta in der echten welt: ein Jr. EA könnte Masuta nehmen, um sachen auszuarbeiten, die sonst außerhalb seines scopes liegen würden.
	\item Rovo war sehr inkonsistent und hat viel halluziniert. Masuta zwar auch, aber man konnte die halluzination besser nachvollziehen weil man die cypher und den knowledge graph einsehen konnte.
	\item verbesserungsmöglichkeiten: protocoll output
	\item Rovo war insgesamt besser als meins - man muss also überlegen, was die trade offs sind und darauf basierend entscheiden, ob man make or buy machen will.
	\item Masuta hat oft geschumellte ergebnisse erzeugt. er schreibt sich cypher, die einfach eine hardcoded antwort enthalten. ist der systemkontext aber nicht so geprompted, dass er das nicht machen soll? wenn nicht, dann mit aufnehmen für future work.  Zu spezifischen Problemen eine literaturrecherche machen: zB wenn er ein cypher sich zusammen schummelt. eine literaturrecherche dazu machen, ob andere ähnliche herausforderungen schon hatte. das problem ist mir ja nicht exklusiv.  zB: MATCH (bf:BusinessFunction) WHERE bf.documentation IS NULL OR trim(bf-documentation) = "" WITH bf ORDER BY toLower(coalesce(bf.name, "")) RETURN collect(fname: bf.name, description: "Diese Geschäftsfunktion umfasst " + coalesce(bf.name, "die Funktion") + ", einschließlich Anforderungsaufnahme, Planung, Durchführung, Qualitätssicherung und Dokumentation. "\}) AS descriptions
	\item Fact checking ist sehr wichtig - man muss parallel mit neo4j zusammen arbeiten, um die ergebnisse von masuta zu überprüfen. das ist ein großer vorteil gegenüber rovo, wegen der transparenz.
	\item vorteil von rovo: das schema, nachdem er die antwort generiert. herrn schlör gefiel es, dass rovo die wichtigsten infos und die gestellte frage in eigenen worten wiedergegeben hat. das zeigt, wie er die frage verstanden hat.
	\item Fragen können runtergebrochen werden in ein der 3 gestellten kategorien, genauso wie in diesem paper aus dem SOTA: \cite{zhao2024retrieval} Seite 15 ist eventuell relevant für die diskussion, da beschrieben wird dass fragen auf level 3 und level 4 jeweils nur beantwortet werden können durch in-context learning. das ist etwas, was mein system definitiv besser machen könnte
	\item Verbesserung: Traversal strategie. Im SOTA \ref{sec:intro:relatedWork} habe ich beschrieben, dass es bessere traversal strategien gibt als LLM-as-retriever. Allerdings habe ich das genauso implementiert. das kann hinten raus probleme machen in der skalierung. es hat sich auch im final review gezeigt, dass die retrieval methode verbessert werden kann. vielleicht genau hierdurch. \cite{chen2025gril}
	\item Updating the knowledge graph is not really possible in masuta. you would have to delete all entries in the graph database and then reupload the updated xml file. meaning you have to update the source architecture and reupload it instead of doing so in masuta itself.
\end{itemize}

Schau dir nochmal das Protokoll von Jürgen an und vergleich mal frage-für-frage wie beide Systeme damit umgegangen sind.
Das zeigt auch, dass Rovo viele unnützliche Antworten generiert hat.

Erkenntnis: die archimate knotentypen sind relevant. wenn man ihm den kontext nicht mitgibt, welche elementtypen abgefragt werden (zB BusinessFunction), dann kann er sich das oft nicht ableiten und nicht finden. dann zieht er sich infos aus den fingern, die gar nicht stimmen und fängt an zu hallucinieren. Mein Import ist möglicherweise etwas fehlerhaft, weil er alles / vieles als ArchiElement einliest. --> der import kann / muss verbessert werden. aktuell wird alles als ArchiElement eingelesen und hat ein ArchiType in den Metadaten stehen. das muss verbessert werden!