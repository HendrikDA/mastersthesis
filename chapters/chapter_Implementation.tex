\chapter{Implementation}
\label{ch:implementation}
The methodology chapter answers the question of what was built and why. This chapter contains all information on what the finished \prototype{} prototype is, including its architecture and a sequence diagram, data processing mechanisms, and features.

%%%%%%%%% Do NOT evaluate in this chaper. Only describe what was done.

%%%%
% Data Used
%%%%
\section{Data Used}
\label{sec:dataUsed}
Describe the SpeedParcel dataset here and how you used it to prepare for the real-world dataset that will be coming in.



%%%%
% Finished Architecture and Prototype
%%%%
\section{Finished Architecture and Prototype}
\label{sec:architecture}
Figure \ref{fig:architecture_diagram} provides a holistic overview of the system architecture underlying the developed prototype. The user (1) starts the interaction by inputting their prompt through a web-based frontend implemented in React (2). The frontend serves as the primary interface for the user to interact with the application and its underlying data. Interaction is done by prompting via natural text. This makes up the presentation layer of the architecture.

Incoming requests are forwarded to the Node.js backend (3) which orechestrates the interaction between the frontend the LLM (4), and the MCP Server (5). The LLM is used to turn the prompt into cyphers and vice versa to turn the cypher's result back into natural language. This is the only external service of the entire architecture. To enable standardized access to the neo4j datbases (6 and 7), the MCP server sits as an intermediary between the application logic and the databases. These components constitute the orchestration layer of the architecture.

The MCP server is connected to both neo4j databases. The first being the playground database (6) which contains textbook data and the user's custom data that they upload (8). The second database is the SpeedParcel database (7) which contains the textbook data and the SpeedParcel example data (9). The separation of the two databases allows the user to toggle between their own playground instance, containing their own data, and the SpeedParcel instance, containing example data that is ready to be prompted. The databases make up the data access layer of the architecture.

Each application component, with the exception of the external service from OpenAI, has a Docker icon at the top left. This indicates that this application component is part of the containerization. Each component runs in its own container.

\begin{figure}[h]
\centering
\makebox[\textwidth][c]{%
  \includegraphics[width=1\textwidth]{images/architectur_diagram}
}
\caption{An architecture diagram showing each component and which components interact with one another. The Docker icon also indicates if the component is containerized.}
\label{fig:architecture_diagram}
\end{figure}

%%%%
% Component Details
%%%%
\section{Component Details}
\label{sec:component_details}

%% Todo: describe the backend in more detail. e.g. how it is prompoted, etc.

%%% Todo:
Mention that this is a "ephemeral conversation memory" and why that is. We will probably need some kind of source on this. Maybe a test run of the system? Like asking it a question, following it through the entire backend and how the answer is generated? This will have to be done after the sequence diagram. Also more depth on each system component. maybe make a subchapter per architecture layer and describe things in more detail there.

%%%%
% Finished Prototype
%%%%
\section{Data Flow}
\label{sec:finishedPrototype}
TThe sequence of interactions and how data flows between each component is visualized in the sequence diagram in figure \ref{fig:sequence_diagram}. After the user enters their prompt into the frontend, the prompt is forwarded to the backend. The backend then begins preprocessing the request by retrieving the current schema information. This information is vital for the LLM in order for it to know how the knowledge-graph is structured and how to create the cyphers for it. Once the backend receives the schema information, the LLM converts the user's prompt into cyphers using all information that it has receives via the context, schema, and prompt. The cyphers are then used to query the selected knowledge-graph stored in the neo4j server. The neo4j server returns raw data to the MCP server, which passes it back to the backend. The backend then converts this raw response back into natural language using the LLM. Once converted, the backend saves the user's prompt and the system's response into the chat history before returning the response to the frontend where it is displayed.


%%%% Todo: Maybe number the individual requests / responses?
\begin{figure}[h]
\centering
\makebox[\textwidth][c]{%
  \includegraphics[width=1\textwidth]{images/sequence_diagram.png}
}
\caption{A UML sequence diagram showing the main flow of data when a user prompts the system.}
\label{fig:sequence_diagram}
\end{figure}


%%%%
% XML Transpiler
%%%%
\section{XML Transpiler}
\label{sec:xmltranspiler}
Explain in detail here how the XML transpiler takes an XML file as the input and transpiles it into Cypher. it is model-to-model and thus touches on the subjects compiler construction, model-driven engineering, graph databases, and enterprise architecture tooling.

Show how fine-tuning the system prompt can have an effect on the results. E.g. if in the context it says to answer within 1-2 sentences or to answer in 4-6 sentences. show examples of how small things in the prompt can have a large impact.

%%%%
% Querying the database agnostically
%%%%
\section{Generating Text-to-Cypher Independent of the Database Schema}
\label{sec:agnostic_cypher}
Highlight this as the main challenge of the thesis! 

This source describes how and why agnostic cyphers can and should be generated. \cite{wan2025prompting} also, check their sources and use those as well. It also mentions that a challenge when giving the LLM the necessary context is that it gets overloaded with information and that the context length may get exceeded (page 5).

This source \cite{mihindukulasooriya2023text2kgbench} talks about how natural language text can be used to \textbf{create} the knowledge graph. Def use this when explaining how i created my speedparcel imports.

The whole system prompting this is called "in-context learning (ICL)". search for sources that support this.


The source from Wan i \ref{sub:agnostic_cypher}  \cite{wan2025prompting} explains how he created a 3-step-preprocessing in order to query the database agnostically. what i did is not 1-to-1 the same thing, but i borrowed the ideas. the main change being changing the CALL db.schema.visualization() from before to the APOC call apoc.meta.schema() which apparently returns more sensible information. that combined with the SHOW INDEXES call give a better result (i assume - i'm writing this before testing just to get my ideas out of my head lol have fun rewriting this. i wrote this in Bremen on 28.12 xoxo)


Does the APOC XML parser create noise in the database? you know how the XML files have view-data in it on how to display it in archi? we're not leaving that out. does that get saved into the database?
