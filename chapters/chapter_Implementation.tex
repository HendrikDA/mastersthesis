\chapter{Implementation}
\label{ch:implementation}
The methodology chapter answers the question of what was built and why. This chapter contains all information on what the finished \prototype{} prototype is, including its architecture and a sequence diagram, data processing mechanisms, and features.

%%%%%%%%% Do NOT evaluate in this chaper. Only describe what was done.

%%%%
% Data Used
%%%%
\section{Data Used}
\label{sec:dataUsed}
Describe the SpeedParcel dataset here and how you used it to prepare for the real-world dataset that will be coming in.


%%%%
% Finished Architecture and Prototype
%%%%
\section{Finished Architecture and Prototype}
\label{sec:architecture}
Figure \ref{fig:architecture_diagram} provides a holistic overview of the system architecture underlying the developed prototype. The user (1) starts the interaction by inputting their prompt through a web-based frontend implemented in React (2). The frontend serves as the primary interface for the user to interact with the application and its underlying data. Interaction is done by prompting via natural text. This makes up the presentation layer of the architecture.

Incoming requests are forwarded to the Node.js backend (3) which orechestrates the interaction between the frontend the LLM (4), and the MCP Server (5). The LLM is used to turn the prompt into cyphers and vice versa to turn the cypher's result back into natural language. This is the only external service of the entire architecture. To enable standardized access to the neo4j datbases (6 and 7), the MCP server sits as an intermediary between the application logic and the databases. These components constitute the orchestration layer of the architecture.

The MCP server is connected to both neo4j databases. The first being the playground database (6) which contains textbook data and the user's custom data that they upload (8). The second database is the SpeedParcel database (7) which contains the textbook data and the SpeedParcel example data (9). The separation of the two databases allows the user to toggle between their own playground instance, containing their own data, and the SpeedParcel instance, containing example data that is ready to be prompted. The databases make up the data access layer of the architecture.

Each of these application components are explained in more detail in section \ref{sec:component_details}. With the exception of the external service from OpenAI, each component has a Docker icon at the top left. This indicates that the application component is part of the containerization. Each component runs in its own container.

\begin{figure}[h]
\centering
\makebox[\textwidth][c]{%
  \includegraphics[width=1\textwidth]{images/architectur_diagram}
}
\caption{An architecture diagram showing each component and which components interact with one another. The Docker icon also indicates if the component is containerized.}
\label{fig:architecture_diagram}
\end{figure}

%%%%
% Component Details
%%%%
\section{Component Details}
\label{sec:component_details}
This section describes each component in more detail with the goal of clarifying why the individual components are implemented the way that they are.

% Frontend
\subsection{Frontend}
\label{sub:frontend}
The frontend of the prototype is implemented as a single-page application using React with TypeScript. React was chosen due to its component-based architecture, which promotes modularity, reusability, and maintainability. TypeScript adds static typing to the JavaScript codebase, reducing runtime errors and improving development reliability in a growing application. For the user interface, the Chakra UI component library is implemented. Chakra UI provides a set of accessible, pre-styled components that significantly reduce the need for custom CSS while ensuring visual consistency across the application. This choice enabled rapid prototyping and allowed the development effort to focus on functionality rather than low-level UI styling concerns.

The frontend is containerized and exposed via \texttt{http://localhost:3000/}. It communicates with the Node.js backend through a set of RESTful HTTP endpoints. Each request contains the userâ€™s prompt as well as a session identifier, which is generated when the page is loaded or when the active database is toggled by the user. The session identifier is used by the backend to maintain chat state and conversation history. By regenerating the session identifier on page reloads and database switches, the system ensures a clear separation of chat contexts. This prevents unintended carry-over of conversation history between different databases or interaction sessions, thereby preserving data consistency and analytical validity.

Further details on the available frontend features and user interactions are provided in Appendix \ref{appendix:instructions:features}.

% Backend
\subsection{Backend}
\label{sub:backend}
The backend contains the entire application logic and is the most crucial part of the prototype. Here, requests are received from the frontend and processed before querying the data from the knowledge-graph and subsequently returning it to the frontend.

% Schema Information
\subsection{Schema Information}
\label{sub:schema}
Before the LLM is able to generate any meaningful cypher, it is necessary to get the knowledge-graph's schema. The schema is a machine-readable summary of the graph structure containing information such as the node types and its relationships to other nodes. This is done by calling \texttt{apoc.meta.schema()}. Directly after getting the schema, the backend also queries the indexes present in the knowledge-graph. This enables the database to lookup nodes and relationships based on specific labels and properties. % Todo, add this source for apoc.meta.schema: https://neo4j.com/labs/apoc/4.1/overview/apoc.meta/apoc.meta.schema/ and this source forx indexes: https://neo4j.com/docs/cypher-manual/current/indexes/

This information is vital for the system prompt in order to be able to build a reliable cypher and return sufficient results.

% System Prompt
\subsection{System Prompt}
\label{sub:prompt}
The system prompt is one of the most crucial parts of the entire application. This is where the LLM is given its context in order to understand how to transform the natural language inputs into the cyphers and how to turn the raw cypher response back into natural language for the response to the user. The actual system prompts can be found in the chapter \ref{appendix:code:prompts} of the Appendix.

The incoming user prompt requires a finely tuned, well thought out system prompt. The goal of the first system prompt is to provide the LLM with all of the context that it needs in order to generate the specific cypher required to fulfil the user's prompt. It contains 4 sections, categorized to help generate the cypher. The first category in the system prompt is a set of rules on how it may generate the cypher, e.g. to infer meaning from the user's prompt to identify which labels in the knowledge-graph to query. The second category is generic schema information and what type of cyphers it may use to fulfil different types of user requests. The third category in the system prompt is the task that the LLM is given, i.e. to generate exactly one cypher, only cypher, and no further text or explanations. The fourth and last category is a set of cypher syntax rules. These were added iteratively during development as the cyphers generated regularly had errors and required assistance to avoid.

This system prompt, along with the schema information, the user's current prompt, and the previous prompt and corresponding system answer are then passed to Open AI's gpt-5 LLM. The returned result is a single cypher which can be directly used to query the knowledge-graph in order to fulfil the user's prompt.

The second system prompt supports in turning the cypher's response back into natural language. It is more straightforward and only contains two blocks. The first block tells the LLM to act as a senior enterprise architect supporting a junior enterprise architect. The second block gives the LLM more information on how to structure the response, e.g. to generally keep the answer within 1-6 sentences, depending on the complexity of the question.

This second system prompt, along with the original user prompt and the cypher's response are again passed to Open AI's gpt-5 LLM. The returned result is then a natural-language response containing the answer to the user's prompt. This is the final result that is returned to the frontend.

Tuning these system prompts greatly alters the quality of results of the entire system. While the second system prompt was rather straightforward, the system prompt for incoming user prompts is the result of fine tuning it during development in all four action research cycles, as described in section \ref{sec:action_research_cycles}. Any less information in this system prompt and the generated cypher's ability to retrieve the required information worsens.

% Ephemeral Conversation Memory
\subsection{Ephemeral Conversation Memory}
\label{sub:prompt}
\prototype{} has an ephemeral conversation memory, meaning that once the session concludes, the content is discarded \cite{zeppieri2025mmag}. This is where the session ID, which is generated and sent from the frontend comes into play. The backend logic holds an array of past user prompts and system answers. These are tied to the session ID. An incoming request's session ID from the frontend is compared to the current session ID stored in the backend. If the session IDs match, then the previous user prompt and system answer are used as further context for the new user prompt. This enables the user to build upon their previous prompt or the system's previous answer, which may be necessary during complex, multi-step interactions \cite{zeppieri2025mmag}.

As the prototype stands, it saves only the most previous user prompt and corresponding system response. However, this can be scaled to contain more of the previous prompts and responses. It was kept to a minimum for this prototype as this can bloat the prompt passed to the LLM and quickly exceed Open AI's character limit as well as increase the amount of time required for it to generate a response.

% Open AI
\subsection{Open AI}
\label{sub:openai}


% MCP
\subsection{MCP Server}
\label{sub:mcp}

% Databases
\subsection{Neo4j Databases}
\label{sub:datbases}

% Docker
\subsection{Local Environment via Docker}
\label{sub:docker}

%%% Todo:
Mention that this is a "ephemeral conversation memory" and why that is. We will probably need some kind of source on this. Maybe a test run of the system? Like asking it a question, following it through the entire backend and how the answer is generated? This will have to be done after the sequence diagram. Also more depth on each system component. maybe make a subchapter per architecture layer and describe things in more detail there.

%%%%
% Finished Prototype
%%%%
\section{Data Flow}
\label{sec:finishedPrototype}
The sequence of interactions and how data flows between each component is visualized in the sequence diagram in figure \ref{fig:sequence_diagram}. After the user enters their prompt into the frontend, the prompt is forwarded to the backend. The backend then begins preprocessing the request by retrieving the current schema information. This information is vital for the LLM in order for it to know how the knowledge-graph is structured and how to create the cyphers for it. Once the backend receives the schema information, the LLM converts the user's prompt into cyphers using all information that it has receives via the context, schema, and prompt. The cyphers are then used to query the selected knowledge-graph stored in the neo4j server. The neo4j server returns raw data to the MCP server, which passes it back to the backend. The backend then converts this raw response back into natural language using the LLM. Once converted, the backend saves the user's prompt and the system's response into the chat history before returning the response to the frontend where it is displayed.


%%%% Todo: Maybe number the individual requests / responses?
\begin{figure}[h]
\centering
\makebox[\textwidth][c]{%
  \includegraphics[width=1\textwidth]{images/sequence_diagram_new}
}
\caption{A UML sequence diagram showing the main flow of data when a user prompts the system.}
\label{fig:sequence_diagram}
\end{figure}


%%%%
% XML Transpiler
%%%%f
\section{XML Transpiler}
\label{sec:xmltranspiler}
Explain in detail here how the XML transpiler takes an XML file as the input and transpiles it into Cypher. it is model-to-model and thus touches on the subjects compiler construction, model-driven engineering, graph databases, and enterprise architecture tooling.

Show how fine-tuning the system prompt can have an effect on the results. E.g. if in the context it says to answer within 1-2 sentences or to answer in 4-6 sentences. show examples of how small things in the prompt can have a large impact.

%%%%
% Querying the database agnostically
%%%%
\section{Generating Text-to-Cypher Independent of the Database Schema}
\label{sec:agnostic_cypher}
Highlight this as the main challenge of the thesis! 

This source describes how and why agnostic cyphers can and should be generated. \cite{wan2025prompting} also, check their sources and use those as well. It also mentions that a challenge when giving the LLM the necessary context is that it gets overloaded with information and that the context length may get exceeded (page 5).

This source \cite{mihindukulasooriya2023text2kgbench} talks about how natural language text can be used to \textbf{create} the knowledge graph. Def use this when explaining how i created my speedparcel imports.

The whole system prompting this is called "in-context learning (ICL)". search for sources that support this.


The source from Wan \cite{wan2025prompting} explains how he created a 3-step-preprocessing in order to query the database agnostically. what i did is not 1-to-1 the same thing, but i borrowed the ideas. the main change being changing the CALL db.schema.visualization() from before to the APOC call apoc.meta.schema() which apparently returns more sensible information. that combined with the SHOW INDEXES call give a better result (i assume - i'm writing this before testing just to get my ideas out of my head lol have fun rewriting this. i wrote this in Bremen on 28.12 xoxo)


Does the APOC XML parser create noise in the database? you know how the XML files have view-data in it on how to display it in archi? we're not leaving that out. does that get saved into the database?
